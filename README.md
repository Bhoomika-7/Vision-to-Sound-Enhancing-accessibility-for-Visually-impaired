# Vision-to-Sound-Enhancing-accessibility-for-Visually-impaired
Vision to Sound: Enhancing Accessibility for the Visually Impaired" is a project aimed at leveraging advanced machine learning algorithms and embedded systems 
# Introduction
- Overview of the problem statement:
- The "Vision to Sound: Enhancing Accessibility for the Visually Impaired" project aims to improve the daily lives of visually impaired individuals by converting visual information into auditory descriptions. Leveraging advanced technologies, the project focuses on:
- Image Processing and Object Detection: Using convolutional neural networks (CNNs) to analyze and identify objects, scenes, and text from camera-captured images.
- Natural Language Processing (NLP): Generating coherent, contextually relevant descriptions from the analyzed image data to make the information understandable to users.
- Embedded Systems Integration: Developing a compact, portable system with low power consumption for real-time processing, incorporating essential hardware components like microcontrollers, cameras, and audio devices.
- Audio Synthesis and Output: Converting text descriptions into audible speech using text-to-speech (TTS) technology, which is delivered through headphones or speakers.
# Problem Statement
Vision to Sound: Enhancing Accessibility for the Visually Impaired" is a project aimed at leveraging advanced machine learning algorithms and embedded systems 
# Abstract
- Global Impact of Visual Impairments: Over 2.2 billion people globally, including 70 million in India, suffer from vision impairments, significantly affecting daily activities such as reading and navigating environments.
- Project Objective: "Vision to Sound" aims to enhance accessibility for visually impaired individuals by converting visual information into auditory descriptions using machine learning and embedded systems.
- Technology Utilized: The project employs convolutional neural networks (CNNs) to analyze images and Long Short-term Memory (LSTM) to generate descriptions, which are then converted into speech through text-to-speech synthesis.
- Portable and Low-Power Solution: The system is designed for real-time processing and is deployed on a compact, portable platform with low power consumption, delivering auditory information via headphones or speakers.
- Improved Accessibility and Independence: By bridging visual and auditory perception, this innovative system empowers visually impaired individuals, offering enhanced accessibility and significantly improving their quality of life.
# Objective 
Convert visuals to sound using machine learning.
Utilize CNNs for accurate object detection.
Generate contextual descriptions with NLP.
Design a low-power, portable embedded system.
Deliver clear audio through TTS for user independence.
# Literature Survey
- 1.Title of the paper: Image caption generation using Visual Attention Prediction and Contextual Spatial Relation Extraction
Year: 2023
Authors:Reshmi Sasibhooshan, Suresh Kumaraswamy & Santhoshkumar Sasidharan

- 2.Title of the Paper: Image Captioning - A deep learning approach using CNN and LSTM Network
Year: 2023
Authors: Preeti Voditel,Aparna Gurjar,Aakansha Pandey, Akrati Jain, Nandita Sharma

- 3.Title of the Paper: Automatic image captioning combining natural language processing and deep neural networks
Year: 2023
Authors: Antonio M. Rinaldi, Cristiano Russo, Cristian Tommasino

- 4. Title of the paper: A Comprehensive Survey of Deep Learning for Image Captioning
Year: 2019
Authors: MD. Zakir Hossain, Ferdous Sohel, Mohd Fairuz Shiratuddin, and Hamid Laga



